---
title: Caffe--多GPU训练流程分析
date: 2017-05-03 19:00:00
categories: fcDLF
---

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

### 一、多GPU训练大体流程图

<center><img src="{{ site.baseurl }}/images/pdDLF/caffe_s2_1.png"></center>

### 二、源码流程分析

1)先从prototxt文件中通过第三方库protobuf读取solver的参数：

{% highlight cpp %}
  caffe::SolverParameter solver_param;
  caffe::ReadSolverParamsFromTextFileOrDie(FLAGS_solver, &solver_param);
{% endhighlight %}

2)在GPU模式下，当检测到GPU时，获取每个GPU的信息，并将0号GPU设为主GPU：

{% highlight cpp %}
  get_gpus(&gpus);
  if (gpus.size() == 0) {
    ...
  } else {
    ...
#ifndef CPU_ONLY
    cudaDeviceProp device_prop;
    for (int i = 0; i < gpus.size(); ++i) {
      cudaGetDeviceProperties(&device_prop, gpus[i]);
      LOG(INFO) << "GPU " << gpus[i] << ": " << device_prop.name;
    }
#endif
    solver_param.set_device_id(gpus[0]);
    Caffe::SetDevice(gpus[0]);
    Caffe::set_mode(Caffe::GPU);
    Caffe::set_solver_count(gpus.size());
  }
{% endhighlight %}

3)当使用的GPU个数大于1时，会使用P2PSync来管理，否则直接由solver训练：

{% highlight cpp %}
  if (gpus.size() > 1) {
    caffe::P2PSync<float> sync(solver, NULL, solver->param());
    sync.Run(gpus);
  } else {
    LOG(INFO) << "Starting Optimization";
    solver->Solve();
  }
{% endhighlight %}

4)多GPU下使用P2PSync管理训练，P2PSync会构建GPU树，并构建P2P通道：

{% highlight cpp %}
template<typename Dtype>
P2PSync<Dtype>::P2PSync(shared_ptr<Solver<Dtype> > root_solver,
                        P2PSync<Dtype>* parent, const SolverParameter& param)
    : GPUParams<Dtype>(root_solver, param.device_id()),
      parent_(parent),
      children_(),
      queue_(),
      initial_iter_(root_solver->iter()),
      solver_() {
#ifndef CPU_ONLY
  ...
  if (parent == NULL) {
    solver_ = root_solver;
  } else {
    Caffe::set_root_solver(false);
    solver_.reset(new WorkerSolver<Dtype>(param, root_solver.get()));
    Caffe::set_root_solver(true);
  }
  this->configure(solver_.get());
  solver_->add_callback(this);

  if (parent) {
    // Enable p2p access between devices
    const int peer = parent->solver_->param().device_id();
    int access;
    CUDA_CHECK(cudaDeviceCanAccessPeer(&access, self, peer));
    if (access) {
      CUDA_CHECK(cudaDeviceEnablePeerAccess(peer, 0));
    } else {
      LOG(INFO)<< "GPU " << self << " does not have p2p access to GPU " << peer;
    }
    // Allocate receiving buffer on parent
    ...
  }
  CUDA_CHECK(cudaSetDevice(initial_device));
#else
  NO_GPU;
#endif
}
{% endhighlight %}

5)为每一个子GPU创建一个P2PSync对象以共享网络:

{% highlight cpp %}
template<typename Dtype>
void P2PSync<Dtype>::Run(const vector<int>& gpus) {
  vector<shared_ptr<P2PSync<Dtype> > > syncs(gpus.size());
  Prepare(gpus, &syncs);
  ...
}
{% endhighlight %}

6)为每个GPU的训练开启子线程，分别调度训练。而当前线程则去调主GPU去处理根solver的训练:

{% highlight cpp %}
template<typename Dtype>
void P2PSync<Dtype>::Run(const vector<int>& gpus) {
  ...
  for (int i = 1; i < syncs.size(); ++i) {
    syncs[i]->StartInternalThread();
  }
  // Run root solver on current thread
  solver_->Solve();
  for (int i = 1; i < syncs.size(); ++i) {
    syncs[i]->StopInternalThread();
  }
}
// P2PSync的线程入口，调用被分配到solver进行训练
template<typename Dtype>
void P2PSync<Dtype>::InternalThreadEntry() {
  ...
  solver_->Step(solver_->param().max_iter() - initial_iter_);
}
{% endhighlight %}

7)在每个线程进入到Solve()中即开始训练，每次迭代中在进入前向反向运算前，通过回调函数将训练网络参数同步到子GPU:

{% highlight cpp %}
callbacks_[i]->on_start();
{% endhighlight %}

{% highlight cpp %}
template<typename Dtype>
void P2PSync<Dtype>::on_start() {
  。。
  // Wait for update from parent
  if (parent_) {
    P2PSync<Dtype> *parent = queue_.pop();
    CHECK(parent == parent_);
  }
  // Update children
  for (int i = children_.size() - 1; i >= 0; i--) {
    Dtype* src = data_;
    Dtype* dst = children_[i]->data_;
    ...
    CUDA_CHECK(cudaMemcpyAsync(dst, src, size_ * sizeof(Dtype),
        cudaMemcpyDeviceToDevice, cudaStreamDefault));
    CUDA_CHECK(cudaStreamSynchronize(cudaStreamDefault));
    children_[i]->queue_.push(this);
  }
}
{% endhighlight %}

8)各个子GPU各自训练:

{% highlight cpp %}
for (int i = 0; i < param_.iter_size(); ++i) {
  loss += net_->ForwardBackward();
}
{% endhighlight %}

9)各子GPU训练结果即梯度汇总到主GPU:

{% highlight cpp %}
callbacks_[i]->on_gradients_ready();
{% endhighlight %}

{% highlight cpp %}
template<typename Dtype>
void P2PSync<Dtype>::on_gradients_ready() {
#ifndef CPU_ONLY
  ...
  // Sum children gradients as they appear in the queue
  for (int i = 0; i < children_.size(); ++i) {
    P2PSync<Dtype> *child = queue_.pop();
    Dtype* src = child->parent_grads_;
    Dtype* dst = diff_;
    ...
    caffe_gpu_add(size_, src, dst, dst);
  }
  // Send gradients to parent
  if (parent_) {
    Dtype* src = diff_;
    Dtype* dst = parent_grads_;
    ...
    CUDA_CHECK(cudaMemcpyAsync(dst, src, size_ * sizeof(Dtype),  //
        cudaMemcpyDeviceToDevice, cudaStreamDefault));
    CUDA_CHECK(cudaStreamSynchronize(cudaStreamDefault));
    parent_->queue_.push(this);
  } else {
    // Loss functions divide gradients by the batch size, so to compensate
    // for split batch, the root solver divides by number of solvers.
    caffe_gpu_scal(size_, Dtype(1.0 / Caffe::solver_count()), diff_);
  }
#endif
}
{% endhighlight %}

10)最后调用 ApplyUpdate() 更新网络参数。再进入下一次迭代。

### 三、多GPU交互训练的核心类P2PSync

P2PSync主要是在多个本地GPU之间使用map-reduce来同步数据，该类继承于GPUParams和Solver的Callback。其中GPUParams是管理存放在GPU端的数据，Callback使solver训练中可以用回调的方式来调用P2PSync相关函数来进行数据同步操作。

{% highlight cpp %}
// 在本地gpu之间使用map-reduce同步数据并行
template<typename Dtype>
class P2PSync : public GPUParams<Dtype>, public Solver<Dtype>::Callback,
    public InternalThread {
 public:
  explicit P2PSync(shared_ptr<Solver<Dtype> > root_solver,
                   P2PSync<Dtype>* parent, const SolverParameter& param);
  ...
  // 运行，树的构建、开线程，开始训练
  void Run(const vector<int>& gpus);
  // 对GPU分组，并构建GPU树
  void Prepare(const vector<int>& gpus,
               vector<shared_ptr<P2PSync<Dtype> > >* syncs);
  inline const int initial_iter() const { return initial_iter_; }

 protected:
  // 在solve中进行前向之前，用于同步网络参数给子GPU
  void on_start();
  // 将子GPU的梯度汇总到主GPU
  void on_gradients_ready();
  // 内部线程入口
  void InternalThreadEntry();

  P2PSync<Dtype>* parent_;
  vector<P2PSync<Dtype>*> children_;
  BlockingQueue<P2PSync<Dtype>*> queue_;
  const int initial_iter_;
  Dtype* parent_grads_;
  shared_ptr<Solver<Dtype> > solver_;
  // 网络参数和梯度是需要跨GPU更新同步的。
  using Params<Dtype>::size_; // Size of buffers
  using Params<Dtype>::data_; // Network parameters
  using Params<Dtype>::diff_; // Gradient
};

{% endhighlight %}

{% highlight python %}
class node:
    def __init__(self, data, next=None):
        self.data = data
        self.next = next
{% endhighlight %}