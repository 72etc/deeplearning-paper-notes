---
title: EM期望最大化
date: 2015-01-01 09:00:00
categories: fbClass
---

### 概述

<center><img src="{{ site.baseurl }}/images/pdBase/class_em1.png"></center>

<center><img src="{{ site.baseurl }}/images/pdBase/class_em2.png"></center>

--- 

<strong>缺点： </strong>

1. “坏”的参数初始值设置可以导致EM算法陷进一些局部最优点； 

2. EM算法的收敛速度比较慢。 只有在不存在直接解决的算法的情况下，才应考虑使用EM算法，因为有它并不是解决限制条件下优化问题的高效方法。

ps：与EM算法关联算法：解高斯混合背景模型、隐马尔可夫模型中学习问题求解、K均值聚类思想。

---

### EM算法和K-means算法的关系

   对应于K-means来说就是我们一开始不知道每个样例x对应隐含变量也就是最佳类别c。最开始可以随便指定一个c给它，然后为了让P(x,y)最大（这里是要让J最小），我们求出在给定c情况下，J最小时的μ（前面提到的其他未知参数），然而此时发现，c可以有更好的（质心与样例x距离最小的类别）指定给样例x，那么c得到重新调整，上述过程就开始重复了，直到没有更好的c指定。这样从K-means里我们可以看出它其实就是EM的体现，E步是确定隐含类别变量，M步更新其他参数来使J最小化。这里的隐含类别变量指定方法比较特殊，属于硬指定，从k个类别中硬选出一个给样例，而不是对每个类别赋予不同的概率。总体思想还是一个迭代优化过程，有目标函数，也有参数变量，只是多了个隐含变量，确定其他参数估计隐含变量，再确定隐含变量估计其他参数，直至目标函数最优。

