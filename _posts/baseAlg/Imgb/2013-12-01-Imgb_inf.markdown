---
title: 信息论
date: 2015-01-01 09:00:00
categories: fbImgb
---

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

### 熵的概念

<center><img src="{{ site.baseurl }}/images/pdBase/imgb_inf1.png"></center>

   随机变量越随机，越是难预测和非结构化，它的熵就越大。假设一个概率接近于1，其他的概率接近于0。那么该随机变量就没有什么随机性，他的熵就更小。如果所有概率相等，那么它们都远离0和1，意味着它们的熵较大。

---

### 负熵

   <center><img src="{{ site.baseurl }}/images/pdBase/imgb_inf2.png"></center>
   
---

### 最大熵原则

   最大熵原则：当根据不完整的信息作为依据进行推断，应该由满足分布限制条件的具有最大熵的概率分布推得。（即利用部分信息确定随机变量集合概率分布的方法）
   
   基本思想：1）求满足某些约束的信源事件概率分布时，应使得信源的熵最大； 2）可以使我们依靠优先的数据达到尽可能客观的效果； 3）克服可能引入的偏差。
   
   <center><img src="{{ site.baseurl }}/images/pdBase/imgb_inf3.png"></center>
   
---
	  
### 互信息

   用于描述两个系统间的信息相关性，或者一个系统所包含的另一个系统中信息的多少，或者说互信息I(Y;X)是通过观察系统输入X来决定输出Y的不确定性的度量。它可以用熵来表示：

<center><img src="{{ site.baseurl }}/images/pdBase/imgb_inf4.png"></center>

   其中微分熵h(X)是在观察系统输出Y之前关于系统输入X的不确定性，而条件微分熵h(X|Y)是在观察了系统输出Y之后的系统输入X的不确定性。这一熵差称为系统输入X和系统输出Y之间的互信息，若其互信息为0，则X和Y是统计上独立的。
   
   两个连续随机变量X和Y之间的互信息具有三个重要性质：1）非负性，I(Y;X)>=0； 2）对称性；I(Y;X)=I(X;Y) 3）不变性，在随机变量的可逆变换下互信息是不变的：

   ps：作用于神经网络时，其输入输出相应记为多维向量X、Y；可用互信息进行图像配准，就是寻找一个空间变换关系，使得经过该变换后两幅间的互信息达到最大。
     
---
 
### 最大互信息原则

   从神经系统的输入层观测到的随机向量X到系统的输出层得到的随机向量Y之间的变换应该这样选择：这种变换使得输出层神经元的活动共同最大化关于输入层的神经元的活动的信息。最大化的目标函数是向量X和Y之间的互信息I(Y;X)。
      
---

### 相对熵

   <center><img src="{{ site.baseurl }}/images/pdBase/imgb_inf5.png"></center>
