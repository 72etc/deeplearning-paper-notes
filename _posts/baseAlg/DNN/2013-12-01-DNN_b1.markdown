---
title: 深度学习基本概念
date: 2015-01-01 12:00:00
categories: fbDnn
---

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<!--<img src="http://latex.codecogs.com/gif.latex? a^{i}"/>
<center><img src="{{ site.baseurl }}/images/pdBase/svm_smo1.png"></center>-->

### 概述

   一种基于无监督特征学习和特征层次结构的学习方法。

   <strong>本质：</strong>通过构建多隐层的模型和海量训练数据（可为无标签数据），来学习更有用的特征，从而最终提升分类或预测的准确性。 “深度模型”是手段，“特征学习”是目的。

### 与浅层学习区别

1. 强调了模型结构的深度，通常有5-10多层的隐层节点；

2. 明确突出了特征学习的重要性，通过逐层特征变换，将样本在原空间的特征表示变换到一个新特征空间，从而使分类或预测更加容易。与人工规则构造特征的方法相比，利用大数据来学习特征，更能够刻画数据的丰富内在信息。

### 深度神经网络DNN分以下三类：

1. 前馈深度网络(feed-forward deep networks，FFDN)，由多个编码器层叠加而成,如多层感知机(multi-layer perceptrons, MLP) 、卷积神经网络(convolutional neural networks, CNN)等。

2. 反馈深度网络( feed-back deep networks,FBDN)，由多个解码器层叠加而成，如反卷积网络(deconvolutional networks,，DN)、层次稀疏编码网络(hierarchical sparse coding,，HSC)等。

3. 双向深度网络(bi-directional deep networks,BDDN)，通过叠加多个编码器层和解码器层构成(每层可能是单独的编码过程或解码过程，也可能既包含编码过程也包含解码过程)，如深度玻尔兹曼机(deep Boltzmann machines,DBM)、深度信念网络(deep belief networks,DBN)、栈式自编码器(stacked auto-encoders,SAE)等。

### 传统神经网络的训练方法为什么不能用在深度神经网络（因为BP算法存在的问题）

1. 在反向传播时梯度越来越稀疏：从顶层越往下，误差校正信号越来越小，当层数较多时，梯度甚至会消失。即会出现梯度弥散（diffusion of gradients）的现象。

2. 梯度下降的优化过程就变成非凸优化，比较容易陷入局部极值：尤其是从远离最优区域开始的时候（随机值初始化会导致这种情况的发生）；

3. 一般，我们只能用有标签的数据来训练：但大部分的数据是没标签的，DNN待学习的参数非常的多（Andrew Ng在Google搞的那个有1.15billion个参数），数据不足会导致严重的过拟合现象；


### 深度神经网络（多层的神经网络）一种训练过程（逐层训练的方式，wake-sleep由hinton在2006年提出, 现在一般不用）

<strong>第一步：采用自下而上的无监督学习（相当于BP神经网络中的权值初始化）</strong>

1. 逐层构建单层神经元。

2. 当所有层训练完后，每层采用wake-sleep算法进行调优。每次仅调整一层，逐层调整。这个过程可以看作是一个feature learning的过程，是和传统神经网络区别最大的部分。wake-sleep算法包含wake阶段和sleep阶段：

* wake阶段：认知过程，通过下层的输入特征（Input）和向上的认知（Encoder）权重产生每一层的抽象表示（Code），再通过当前的生成（Decoder）权重产生一个重建信息（Reconstruction），计算输入特征和重建信息残差，使用梯度下降修改层间的下行生成（Decoder）权重。也就是“如果现实跟我想象的不一样，改变我的生成权重使得我想象的东西变得与现实一样”。

* sleep阶段：生成过程，通过上层概念（Code）和向下的生成（Decoder）权重，生成下层的状态，再利用认知（Encoder）权重产生一个抽象景象。利用初始上层概念和新建抽象景象的残差，利用梯度下降修改层间向上的认知（Encoder）权重。也就是“如果梦中的景象不是我脑中的相应概念，改变我的认知权重使得这种景象在我看来就是这个概念”。 

---

<strong>第二步：自顶向下的监督学习（用wake-sleep + 梯度下降法逐层分别各自调整，而不是用BP一直往回传）</strong>

这一步是在第一步学习获得各层参数进的基础上，在最顶的编码层添加一个分类器（例如罗杰斯特回归、SVM等），而后通过带标签数据的监督学习，利用梯度下降法去微调整个网络参数。

深度学习的第一步实质上是一个网络参数初始化过程。区别于传统神经网络初值随机初始化，深度学习模型是通过无监督学习输入数据的结构得到的，因而这个初值更接近全局最优，从而能够取得更好的效果。

---

好处：可通过学习一种深层非线性网络结构，实现复杂函数逼近，表征输入数据分布式表示。

难题（缺点）：

1. 深度神经网络模型复杂，训练数据多，计算量大。	

2. 深度神经网络训练收敛难，需要反复多次实验。深度神经网络是非线性模型，其代价函数是非凸函数，容易收敛到局部最优解。
				
3. 深度神经网络的模型结构、输入数据处理方式、权重初始化方案、参数配置、激活函数选择、权重优化方法等均可能对最终效果有较大影响。
				
4. 深度神经网络的数学基础研究稍显不足。虽然可以通过限制性波尔兹曼机（Restricted Boltzmann Machines，RBMs）等减少陷入局部最优的风险，但仍然不是彻底的解决方案。

<center><img src="{{ site.baseurl }}/images/pdBase/dnn_b1.png"></center>

