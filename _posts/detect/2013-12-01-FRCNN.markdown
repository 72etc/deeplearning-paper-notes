---
title: Fast-RCNN（ICCV，2015）
date: 2016-01-04 19:50:00
categories: fDetect
---

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

代码：[https://github.com/rbgirshick/fast-rcnn](https://github.com/rbgirshick/fast-rcnn)

论文：Girshick R. Fast R-CNN[C]// IEEE International Conference on Computer Vision. IEEE, 2015:1440-1448.

### 论文算法概述

       该算法针对RCNN和SPP-Net中的缺点而提出，具有以下优点：1）比RCNN和SPP-Net更高的检测率； 2）训练过程简洁，使用多任务loss；  3）能更新所有网络层（SPP无法更新卷积层）； 4）改进了SPP-Net后面的需要将第二层FC的特征放到硬盘上训练SVM，之后再额外训练bbox regressor。
     训练过程为将完整图像输入到各卷积层和池化层中得到特征图，把最后一层特征图中每个候选框的相应区域输入到单层（only one pyramid level）的SPP layer（文中称为ROI池化层）中，以使每个候选框得到固定长度的特征向量。特征向量输入到全连接层中而全连接层后面接两个输出层，一个输出各类的softmax概率，另一个输出各类对应的四个实数表示目标方框位置；


<center><img src="{{ site.baseurl }}/images/pdDetect/frcnn1.png"></center>

### Roi池化层（单层金字塔的SPP空间金字塔池化层）

   使用极大值池化，将感兴趣区域的特征转化到一个小的固定尺寸（H*W）的特征图中，H和W作为超参数独立于各个特定的感兴趣区域ROI。这里的ROI指的是卷积特征图的一个矩形窗口（h*w），而极大值池化核大小约为（h/H * w/W），在每个子窗口中进行池化到H*W中对应的网格中。这ROI池化层是SPP-Net中的特殊例子，与SPP层为多尺度，ROI池化层为单尺度。


### 使用预训练网络进行初始化

   使用预训练网络需要经过三个阶段:1、网络最后的极大值池化层改为ROI池化层去连接第一层全连接层； 2、最后的全连接层和1000类softmax层更换成全连接层和K+1类softmax 再加上 窗口位置信息。 3、网络输入增加输入ROI信息；

### 调优

   FastRCNN只有一个调优阶段，联合优化softmax分类器和bbox回归器；而RCNN和SPPnet包含softmax分类器、SVMs和回归器三个调优阶段。文中基于特征共享提出一种更高效的训练方法，在FRCNN中选取N张图片，从每张图像中提取R/N个ROI，这样ROI来自同一张图片可以在前向和后向中共享计算量和内存。如文中试验采用N=2，R=128，即两张图片各取64个ROI，可以比从128张图片中各提取一个ROI快64倍。因为这里计算是对一张图片进行卷积，后提取ROI做ROI池化等后面操作，主要运算量与图片大小和数量相关联。


### 多任务loss

   FastRCNN有两个输出层，两个loss联合训练：<img src="http://latex.codecogs.com/gif.latex? L\left( {p,u,t^u ,v} \right) = L_{cls} (p,u) + \lambda [u \ge 1]L_{loc} (t^u ,v)"/>，类别为u，框为v。一个输入K+1类的概率分布，使用softmaxLoss：<img src="http://latex.codecogs.com/gif.latex? L_{loc}  =  - \log p_u"/>；另一个是输出目标的回归窗口位置，使用的是smoothL1Loss：<img src="http://latex.codecogs.com/gif.latex? L_{loc} (t^u ,u) = \sum\limits_{i \in \{ x,y,w,h\} } {smooth_{L1} (t_i^u ,v_i )} "/>，<img src="{{ site.baseurl }}/images/pdDetect/frcnn2.png">，目的是让loss对离散点更具鲁棒性。若使用L2 loss需要认真调整学习率防止梯度暴增；

### 尺度不变性

   文中探索了两种方式来实现尺度不变的物体检测：1）暴力方法：每张图片都设置为固定大小，使网络直接从训练数据中学习尺度不变性； 2）通过多尺度方法，在测试时利用图像金字塔近似对每个样本进行尺度归一化，在训练时每张图像随机选取金字塔中的一个尺度，这也是数据扩增的方式。文中采用的是方式1，多尺度比单尺度准确率仅高一点点约1%，但速度慢很多。


### 用奇异值分解加速检测过程

   使用Truncated SVD（SVD的变形,只计算用户指定的最大的k个奇异值）对全连接层进行分解，速度可提高30%，而mAp仅下降0.3%，并且在压缩后不需要进行微调。

### 算法设计细节

1. 哪些层需要finetune？答：对于层次较深的网络，只finetune全连接层而不管卷积层会造成较大的性能损耗，如文中试验VGG16只调fc为61.4%，fc和conv一起调为66.9%；但并非所有卷积层都要调，对于浅层的卷积层影响不大，不finetune这些层可以节省训练时间。

2. 多任务的好处？答：相对于RCNN的多个单独训练任务来说方便；而且任务间通过卷积层共享进行相互影响可以提高性能。

3. 为什么用softmax替换SVM？答：实验结果表明softmax的mAP更高，“We note that softmax, unlike one-vs-rest SVMs, introduces competition between classes when scoring a RoI”。

