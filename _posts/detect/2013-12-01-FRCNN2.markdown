---
title: Faster-RCNN（NIPS，2015）
date: 2016-01-06 19:50:00
categories: fDetect
---

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

论文：Ren S, He K, Girshick R, et al. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.[J]. IEEE Transactions on Pattern Analysis & Machine Intelligence, 2016:1-1.

代码matlab：[https://github.com/ShaoqingRen/faster_rcnn](https://github.com/ShaoqingRen/faster_rcnn)

代码python_1：[https://github.com/rbgirshick/py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn)

代码python_2：[https://github.com/rbgirshick/fast-rcnn](https://github.com/rbgirshick/fast-rcnn)

代码C++: [https://github.com/YihangLou/FasterRCNN-Encapsulation-Cplusplus ](https://github.com/YihangLou/FasterRCNN-Encapsulation-Cplusplus)

译文推荐：[http://www.2cto.com/kf/201604/503750.html](http://www.2cto.com/kf/201604/503750.html)

### 论文算法概述

       提出了一种Region Proposal Network (RPN)，共享目的是为了共享fast-rcnn物体检测网络中卷积特征的计算，以实现低运算量提取目标区域，解决Fast-RCNN和SPP-net中候选框提取速度慢的问题。RPN是一个端到端的全卷积网络，以任意大小的图像作为输入，只需在最后的卷积层上滑动一遍，可以同时预测物体的区域和在各区域的objectness scores（“Objectness” measures membership to a set of object classes vs. background），替换selective search应用到fast-rcnn中。RPN同时也提高了候选框的质量，使检测更准确。
   
### RPN   
	
	为产生目标区域，在网络的最后一层共享卷积层（如ZF的第5层或VGG的第13层卷积层）中产生的卷积特征映射图上滑动一个小网络，这个小网络与输入的卷积特征图的n x n的空间滑动窗口进行全连接，滑动窗口每个都映射出一个低维向量，如在ZFnet中最后一层卷积层为256个特征图，则输出256维向量：一个3×3窗口在一个特征图的一个位置上输出一个节点，共256个特征图，则有256个节点，得到1×256的向量，这向量输入到方框回归器reg和分类器cls两个关联的全连接层中（无论有多少个特征图都只滑动一次，特征图个数等于输出向量维度）。文中滑动窗口设为n=3，这小网络滑到一个位置时如图所示。因为这小网络是以滑动窗口的形式使用的，即这全连接的网络层可以被所有空间位置共享。它的网络结构是由n x n的卷积层加上两个滑动的1 x 1的卷积层（for reg和cls）。ReLU应用于n x n卷积层。

<center><img src="{{ site.baseurl }}/images/pdDetect/f2rcnn.png"></center>
	
	更具体地说，该滑动窗口在以下网络上，用步长为1的3x3卷积来实现，即rpn_conv/3x3。256-d为下图中rpn_conv/3x3的输出特征图个数，即每个位置的3x3x256卷积操作得到1x1x256d。
	
	假设rpn_conv/3x3的输出为m x n x 256，外接1x1x18卷积，则输出分类特征图大小为m x n x 18，即特征图上每个点会得到2x9=18维度的向量，表示该位置点上的每种anchor是否含有目标。同理用1x1x36卷积得到m x n x 36边框预测特征图。
	
<center><img src="{{ site.baseurl }}/images/pdDetect/f2rcnn2.png"></center>

### 旋转不变的Anchors

   在每个滑动窗口位置，同时预测k个目标区域，因此reg层输出k个boxes，cls输出2k个分数（是目标的概率和非目标的概率）；每个anchor在滑动窗口的中心，与一个尺度和长宽比相关联。文中采用3个尺度（128,256,512）和3种长宽比例（1:1,1:2,2:1），则在每个滑动位置点上可产生k=9个anchors。一个卷积特征图大小为W x H，则有W x H x k个anchors，具有旋转不变性。

### 损失函数

   在训练RPN时，给每个anchor设置一个二值化类别标签（是/否物体），其中与一个真实物体区域具有最高的Intersection over-Union (IoU)重叠或与任意的真是物体区域的IoU超过0.7的anchor标记为“是”；而与任何物体真实区域的IoU都不超过0.3，则为“否”；介于“是”与“否”之间的舍去不做训练。训练时遵循Fast-RCNN中的多任务loss，损失函数如Fast-RCNN相似，并以与以往不同的方式实现了其中的bounding-box回归，详看论文。

### 训练优化

   RPN是全卷积网络，由方向传播和随机梯度SGD进行端到端训练。每个mini-batch由一张图像上随机抽选的256个正和负样本候选框组成，并使正负样本比例约1：1，否则负样本很多可能会使bias倾向于负样本。使用0均值0.01标准差的高斯分布对所有新网络层进行初始化，而其他层由预训练模型提供，进行finetune。

### 共享卷积特征

   这里检测网络采用的是Fast-RCNN，而RPN和Fast-RCNN的训练相互独立，现在是想让它们之间共享卷积特征以减少预测时的计算量。但Fast-RCNN的训练是基于固定候选框，在训练Fase-RCNN的同时调整候选框则无法确定网络训练是否会收敛，因此设计一个包含有Fast-RCNN和RPN的网络并简单联合使用反向传播训练是不容易实现的。文中采用的方法含4个步骤：

1. 照常训练RPN，使用预训练模型进行初始化并对候选框做端到端finetune。
2. 使用RPN产生的候选框对Fast-RCNN进行训练，Fast-RCNN也采用ImageNet的预训练模型进行初始化。
3. 使用Fast-RCNN网络去初始化RPN的训练，固定卷积层微调RPN独有的网络层。这样两个网络就共享卷积层了。
4. 保持共享卷积层固定，去对Fast-RCNN的全连接层进行finetune。

### 总结

   主要是提出了RPN，代替了Fast-RCNN上的使用的selective-search，其余结构未变。而RPN中设定3个尺度和3个形状组成9个anchors，以滑动窗口的方式搜索目标。其中滑动窗口以n x n卷积实现，卷积输出多个特征图，即使每个滑动窗口得到一个多维向量。然后再用1x1x18和1x1x36卷积去扫描，分别得到对应每个位置上9个anchor的类别和边框的预测。