<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
<title>A-Fast-RCNN（CVPR, 2017）</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="计算机视觉算法学习笔记">
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/hc.css" rel="stylesheet">
<link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
<!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
<!--[if lt IE 9]>
  <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif] d-->

  </head>
  <body>
    <div class="nav-toggle"><i class="fa fa-bars fa-2x"></i> ChenJM </div>
    <div id = "wrapper">
      <div class="navbar navbar-default" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <p class="navbar-brand">陈健明</p>
    </div>
    <div class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
      <!--<li><a href="/list.html">List</a></li>-->
      <!--<li><a href="/links.html">Links</a></li>-->
      <li><a href="/projects.html">Algorithms</a></li>
      <li><a href="/programs.html">Programs</a></li>
      <li><a href="/about.html">About Me</a></li>
      </ul>
    </div><!--/.nav-collapse -->
  </div>
</div>

<!-- Sidebar -->
<div id="sidebar-wrapper">
  <ul class="sidebar-nav">
    <li class="sidebar-brand"><a href="/"><h1 class="brand">陈健明</h1></a><h3>计算机视觉算法学习笔记</h3></li>

    <div>
      <center><img src="/images/me.jpg" height="195" width="180"></center>
    </div>
 
    <hr />
    <li><a href="/projects.html">论文阅读(2015--)</a></li>
    <li><a href="/bprojects.html">基础算法(2013-2015)</a></li>
    <!-- <li><a href="/programs.html">开源码学习</a></li> -->
    <li><a href="/about.html">关于</a></li>
    <hr />

    
    <div>
      <center><img src="/images/weixin.jpg" height="130" width="130"></center>
    </div>
    <!--
    <div id="social-wrapper">
      <li> <a href=""><i class="fa fa-twitter-square"></i> @twitter</a></li>
      <li> <a href=""><i class="fa fa-linkedin-square"></i> linkedin</a> </li>
      <li> <a href=""><i class="fa fa-facebook-square"></i> facebook</a></li>
      <li> <a href=""><i class="fa fa-github-square"></i> github</a> </li>
    </div>
    -->
  </ul>
</div>

      <div class="container">
        <div id="article">
  <div class="article-title">A-Fast-RCNN（CVPR, 2017）</div>
  <p class="meta">
    <small>
      &nbsp;<i class="fa fa-calendar-o"></i> 01 May 2017
    </small>
  </p> <hr/>
  <div class="post">
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>论文： A-Fast-RCNN : Hard Positive Generation via Adversary for Object Detection</p>

<p>Github: <a href="https://github.com/xiaolonw/adversarial-frcnn]">https://github.com/xiaolonw/adversarial-frcnn</a></p>

<h3 id="section">论文算法概述</h3>

<div class="highlighter-rouge"><pre class="highlight"><code>   为了使检测器对遮挡和形变具有一定的不变性，一般用的是数据驱动的方式进行模型训练。但这些遮挡和形变的样本在整个训练集中的比例很小，服从long-tail分布，部分情况会更稀缺，也太不可能会涵盖所有情况。所以文中方案是训练一个对抗网络来生成一些难以被检测器准确分类的遮挡和形变样本，生成的方式是通过掩盖空间上的部分特征图得到遮挡图片或对特征响应作修改生成空间上的形变。而这想法的关键点在于在卷积特征空间生成对抗样本而不是直接生成像素.对抗网络的监督信号是softmax，学习用来预测会使检测器检测错误的特征。因此，如果从对抗网络上得到的特征很容易被检测器正确分类，那该特征在对抗网络上会有较大的loss。

   这里基于Fast RCNN实验，与原版Fast RCNN相比，在VOC2007上mAP提高2.3%，在VOC2012上提高2.6%。
</code></pre>
</div>

<center><img src="/images/pdDetect/afastrcnn1.png" /></center>

<h3 id="adversarial-spatial-dropout-for-occlusion">Adversarial Spatial Dropout for Occlusion</h3>

<p>提出了Adversarial Spatial Dropout Network(ASDN)在前景目标的深度特征上生成遮挡掩膜。把ROI池化后得到的基于区域的特征作为对抗网络的输入，给定一个物体的特征图，ASDN会尝试生成一个mask表示该部分特征被置空后检测器无法准确检测到该物体。例如，给定一个目标提取到特征X，维度是d x d x c，d为空间上的尺度，c为通道数，ASDN会预测一个d x d的mask M，并阈值化为0和1，当M_ij为0时，则特征图上所有通道的(i,j)位置均置为0。</p>

<p><strong>Network Architecture：</strong>Fast RCNN由ImageNet预训练模型进行初始化，而对抗网络则共享fast RCNN中的卷积层和ROI池化层（只是结构不含权重），然后接上自己的全连接层。注意，两个网络进行对抗训练，网络权重不共享。</p>

<p><strong>Model Pre-training：</strong>在实验中发现，在使用ASDN去提升Fast-RCNN之前，以生成occlusions 为目的去预训练ASDN很重要。在这里采用阶段性训练的方式，首先不含ASDN去训练Fast-RCNN，等Fast-RCNN有一定效果时，开始训练ASDN模型去生成occlusions。</p>

<p><strong>Initializing ASDN Network：</strong>为了初始化ASDN模型，给定一个d x d的特征图X，在上面使用一个d/3 x d/3的滑动窗口，如图3(a)。每到一个滑动位置将对应所有通道的值屏蔽掉，生成该区域的一个新的特征向量，然后该特征向量通过分类层去计算loss。将每个滑动窗口，各自屏蔽自己的部分后得到的各个对应特征向量后，都计算loss，选择最高的loss，以该loss对应的窗口为基准，生成一个d x d的mask。为n个正样本候选框生成这些mask，得到n对用于ASDN的训练样本。主要的思想是让ASDN学会去生成这些能让检测器具有较大loss的mask。ASDN的监督信号为二值化的交叉熵函数：</p>

<center><img src="/images/pdDetect/afastrcnn2.png" /></center>

<p>式中A_ij(X)为给定特征图X时ASDN在坐标i,j上的输出，在训练时使用该loss函数迭代10K次后，可以看到这网络可以判断图像上哪部分对分类器的影响大，如图3(b)所示。</p>

<center><img src="/images/pdDetect/afastrcnn3.png" /></center>

<p><strong>Thresholding by Sampling：</strong>ASDN的输出并不是二值化的mask，更像是连续的热力图。这里采用的阈值化方式是截取最高的1/3像素，或者选取最高的1/2然后随机排除1/2中的1/3，以保持多样性和随机性。</p>

<p><strong>Joint Learning：</strong>给定预训练的ASDN和Fast-RCNN模型后，在每次迭代中联合两个模型进行训练。为训练Fast-RCNN检测器，先使用ASDN去生成mask，采样阈值化后应用到前向过程中ROI池化后的特征图上。然后以修改过的特征进行现象传播。特征被修改了，但标签还是一样的，这样相当于生成了更困难更多样化的训练样本。为训练ASDN，因为使用了采样的策略去将热力图转化为二值mask，转化后，结果不可微，所以不能将分类器的loss直接反向传播。所以文中采用的方式是，计算哪个mask对分类影响较大，然后使用这些困难的mask作为标签去直接去训练ASDN，loss使用的是Initializing ASDN Network中提到的二值化交叉熵。</p>

<h3 id="adversarial-spatial-transformer-network">Adversarial Spatial Transformer Network</h3>

<p><strong>STN Overview：</strong>STN有三个组成部分：定位网络localisation network，栅格生成器grid generator和采样器sampler。给定特征图作为输入，localisation network会为这些形变估计变量（如旋转角度，平移距离和缩放因子等）。这些变量都会被用于作为grid generator和sampler的输入运用在特征图上，输出则为形变的特征图。注意这里只需要训练localisation network的参数。STN的一个主要贡献是使整个过程可微分，因此localisation network可以通过反向传播直接去优化分类器。细节参考论文《Spatial transformer networks》.NIPS2015。</p>

<p><strong>Adversarial STN：</strong>在这里的ASTN，主要关注点是特征图的旋转。给定一个经过ROI池化后的特征图作为输入，ASTN会学习去旋转特征图使其更难以被识别。localisation network包含三个全连接层，前面两层由ImageNet的fc6和fc7的预训练参数作为初始值。联合训练ASTN和Fast-RCNN检测器，训练检测器的过程与ASDN中的相似。经过ROI池化后的特征由ASTN进行转换，然后前向传播到更高层去计算softmax损失。ASTN的训练目标是使检测器混淆产生误检，而ASTN的训练与ASDN不同的是，ASTN的空间变换是可微分的，可以直接使用分类的loss去反向传播微调ASTN中的localisation network的参数。</p>

<p><strong>Implementation Details：</strong>在实验中，限制由ASTN产生的旋转角度范围在顺逆时针旋转的10度以内，且并非把全部特征图都旋转到同一方向，而是将特征图基于通道维度分成4块并为每块估计4个不同的角度。因为每个通道对应着一种特征的激活情况，各自旋转通道对应着将物体的旋转各个部分，则会造成形变。作者也发现，如果把所有特征图都旋转同一个角度，ASTN会经常预测得到最大的角度。而如果旋转的是四个角度而不是一个，这样任务的复杂度就高了，会抑制网络预测一些不重要的形变。</p>

<h3 id="adversarial-fusion">Adversarial Fusion</h3>

<p>两个对抗网络ASDN和ASTN可以合并在一起在同一个检测框架中进行训练，各自提供不同的信息，通过两个网络的同时对抗使检测器更具鲁棒性。这里将这两个网络以连续的方式合并到Fast-RCNN中，如图4所示。从ROI池化后提取到的特征图输入到ASDN中屏蔽掉部分神经元，然后把新的特征图再送入ASTN做形变。</p>

<center><img src="/images/pdDetect/afastrcnn4.png" /></center>

<h3 id="experiments">Experiments</h3>

<p>在VOC2007上的结果如下图所示，与Online Hard Example Mining（OHEM）相比，在VOC2007上文中结果（71.4%）比OHEM（69.9%）要好，但在VOC2012上文中结果（69.0%）比OHEM（69.8%）要稍低一点。作者认为这两种方法可以互补，实验中将两种方法联合，在VOC2012上可以达到71.7%；而两个OHEM模型联合为71.2%，两个文中的模型联合则为70.2%。</p>

<center><img src="/images/pdDetect/afastrcnn5.png" /></center>

  </div>
</div>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<!--<a href="https://twitter.com/share" class="twitter-share-button " data-size="small" data-count="none">Tweet</a>
<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
-->
<ul class="pager">

  <li class="previous"><a href="/fdetect/2017/04/30/OHEM.html">&larr; Older</a></li>


  <li class="next"><a href="/fmask/2017/05/11/UDC_HDC.html">Newer &rarr;</a></li>

</ul>

<!--<div id="disqus_thread">

  <script type="text/javascript">
  /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
      var disqus_shortname = 'diqus-username'; // required: replace example with your forum shortname

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
  </script>
  <noscript>
    Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>

  <a href="http://disqus.com" class="dsq-brlink">
    comments powered by <span class="logo-disqus">Disqus</span>
  </a>

</div>
-->

      </div>
      <div class="container">
  <footer>
    <p class="text-muted credit">
      <center>
      &copy; 2017 陈健明 &middot;
      <a href="https://github.com/cjmcv"> Github Site </a>
      2017-10-22 22:32:16 +0800
      </center>
    </p>
  </footer>
</div>

    </div>
    <!-- Bootstrap core JavaScript-->
<script src="/js/jquery-1.10.2.min.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/hc.js"></script>

  </body>
</html>
