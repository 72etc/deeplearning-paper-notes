<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
<title>FCN（CVPR，2015）</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="计算机视觉算法学习笔记">
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/hc.css" rel="stylesheet">
<link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
<!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
<!--[if lt IE 9]>
  <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif] d-->

  </head>
  <body>
    <div class="nav-toggle"><i class="fa fa-bars fa-2x"></i> ChenJM </div>
    <div id = "wrapper">
      <div class="navbar navbar-default" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <p class="navbar-brand">cjmcv</p>
    </div>
    <div class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
      <!--<li><a href="/list.html">List</a></li>-->
      <!--<li><a href="/links.html">Links</a></li>-->
      <li><a href="/projects.html">Algorithms</a></li>
      <li><a href="/programs.html">Programs</a></li>
      <li><a href="/about.html">About Me</a></li>
      </ul>
    </div><!--/.nav-collapse -->
  </div>
</div>

<!-- Sidebar -->
<div id="sidebar-wrapper">
  <ul class="sidebar-nav">
    <li class="sidebar-brand"><a href="/"><h1 class="brand">cjmcv</h1></a><h3>计算机视觉算法学习笔记</h3></li>

    <div>
      <center><img src="/images/me.jpg" height="195" width="180"></center>
    </div>
 
    <hr />
    <li><a href="/projects.html">论文阅读(2015--)</a></li>
    <li><a href="/bprojects.html">基础算法(2013-2015)</a></li>
    <li><a href="/about.html">个人简历</a></li>
    <li><a href="/statement.html">版权声明</a></li>
    <li><a href="https://github.com/cjmcv"> <font color="#00BFFF"> Github Site </font></a></li>
    <hr />


    <div>
      <center><img src="/images/weixin.jpg" height="130" width="130"></center>
    </div>
    <!--
    <div id="social-wrapper">
      <li> <a href=""><i class="fa fa-twitter-square"></i> @twitter</a></li>
      <li> <a href=""><i class="fa fa-linkedin-square"></i> linkedin</a> </li>
      <li> <a href=""><i class="fa fa-facebook-square"></i> facebook</a></li>
      <li> <a href=""><i class="fa fa-github-square"></i> github</a> </li>
    </div>
    -->
  </ul>
</div>

      <div class="container">
        <div id="article">
  <div class="article-title">FCN（CVPR，2015）</div>
  <p class="meta">
    <small>
      &nbsp;<i class="fa fa-calendar-o"></i> 16 Mar 2016
    </small>
  </p> <hr/>
  <div class="post">
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>部分内容摘自：<a href="http://www.2cto.com/kf/201604/503146.html">http://www.2cto.com/kf/201604/503146.html</a></p>

<p>代码和模型：<a href="https://github.com/BVLC/caffe/wiki/Model-Zoo#fcn">https://github.com/BVLC/caffe/wiki/Model-Zoo#fcn</a></p>

<p>算法使用示例代码：<a href="https://github.com/SHUCV/digit">https://github.com/SHUCV/digit</a></p>

<p>论文：Shelhamer E, Long J, Darrell T. Fully Convolutional Networks for Semantic Segmentation[J]. 2014, 79(10):1337-1342.</p>

<h3 id="section">论文算法概述</h3>

<div class="highlighter-rouge"><pre class="highlight"><code>   全卷积网络（FCN）进行像素级的分类从而高效的解决了语义级别的图像分割（semantic segmentation）问题。与经典的CNN在卷积层之后使用全连接层得到固定长度的特征向量进行分类不同，FCN可以接受任意尺寸的输入图像，采用反卷积层对最后一个卷积层的feature map进行上采样, 使它恢复到输入图像相同的尺寸，从而可以对每个像素都产生了一个预测, 同时也保留了原始输入图像中的空间信息, 最后在上采样的特征图上进行逐像素分类。论文中逐像素计算softmax分类的损失, 相当于每一个像素对应一个训练样本。

   Longjon用于语义分割所采用的全卷积网络（FCN)的结构示意图, 在Alexnet基础上, 最后的channel=4096的feature map经过一个1x1的卷积层, 变为channel=21的feature map, 然后经过上采样和crop（由于步长（stride）不为一的卷积层和池化层产生的特征图（feature map）大小会有一些向下取整操作, 导致最后的feature map大小与原图不是严格的倍数关系，crop层进行裁剪）, 变为与输入图像同样大小的channel=21的feature map, 也就是图中的pixel-wise prediction。 在Longjon的试验中一共有20个语义类别, 加上背景类别每个像素应该有21个softmax预测类, 因此pixel-wise prediction中channel=21。
</code></pre>
</div>

<center><img src="/images/pdMask/fcn1.jpg" /></center>

<h3 id="section-1">细节记录</h3>

<h5 id="dense-prediction">dense prediction</h5>

<p>这里通过upsampling得到dense prediction，作者研究过3种方案：</p>

<ul>
  <li>
    <p>1、shift-and-stitch：设原图与FCN所得输 出图之间的降采样因子是f，那么对于原图的每个f<em>f的区域（不重叠），“shift the input x pixels to the right and y pixels down for every (x,y) ,0 &lt; x,y &lt; f.” 把这个f</em>f区域对应的output作为此时区域中心点像素对应的output，这样就对每个f*f的区域得到了f^2个output，也就是每个像素都能对应一个output，所以成为了dense prediction。</p>
  </li>
  <li>
    <p>2、filter rarefaction：就是放大CNN网络中的subsampling层的filter的尺寸，得到新的filter：其中s是subsampling的滑动步长，这个新filter的滑动步长要设为1，这样的话，subsampling就没有缩小图像尺寸，最后可以得到dense prediction。</p>
  </li>
</ul>

<p>以上两种方法作者都没有采用，主要是因为这两种方法都是trad-off的，原因是：对于第二种方法， 下采样的功能被减弱，使得更细节的信息能被filter看到，但是receptive fileds会相对变小，可能会损失全局信息，且会对卷积层引入更多运算。对于第一种方法，虽然receptive fileds没有变小，但是由于原图被划分成f*f的区域输入网络，使得filters无法感受更精细的信息。</p>

<ul>
  <li>3、重点方法:反卷积层-&gt;pixel wise-&gt;bp parameters-&gt;实现把conv的前传和反传过程对调一下即可。这里upsampling的操作可以看成是反卷积(deconvolutional)，卷积运算的参数和CNN的参数一样是在训练FCN模型的过程中通过bp算法学习得到。</li>
</ul>

<h5 id="fusion-prediction">fusion prediction</h5>

<p>以上是对CNN的结果做处理，得到了dense prediction，而作者在试验中发现，得到的分割结果比较粗糙，所以考虑加入更多前层的细节信息，也就是把倒数第几层的输出和最后的输出做一个fusion，实际上也就是加和。这样就得到第二行和第三行的结果，实验表明，这样的分割结果更细致更准确。在逐层fusion的过程中，做到第三行再往下，结果又会变差，所以作者做到这里就停了。可以看到如上三行的对应的结果：</p>

<center><img src="/images/pdMask/fcn2.jpg" /></center>

<h3 id="section-2">问题&amp;解决办法</h3>

<ul>
  <li>
    <p>1.如何做pixelwise的prediction？ 传统的网络是subsampling的，对应的输出尺寸会降低，要想做pixel wise prediction，必须保证输出尺寸。</p>

    <p>解决办法：
  <em>(1)</em> 对传统网络如AlexNet，VGG等的最后全连接层变成卷积层。例如VGG16中第一个全连接层是25088x4096的，将之解释为512x7x7x4096的卷积核，则如果在一个更大的输入图像上进行卷积操作（上图的下半部分），原来输出4096维feature的节点处（上图的上半部分），就会输出一个coarse feature map。这样做的好处是，能够很好的利用已经训练好的supervised pre-training的网络，不用像已有的方法那样，从头到尾训练，只需要fine-tuning即可，训练efficient。 
  <em>(2)</em> 加In-network upsampling layer。对中间得到的feature map做bilinear上采样，就是反卷积层。实现把conv的前传和反传过程对调一下即可。</p>
  </li>
  <li>
    <p>2.如何refine，得到更好的结果？upsampling中步长是32，输入为3x500x500的时候，输出是544x544，边缘很不好，并且limit the scale of detail of the upsampling output。</p>

    <p>解决办法：采用skip layer的方法，在浅层处减小upsampling的步长，得到的fine layer 和 高层得到的coarse layer做融合，然后再upsampling得到输出。这种做法兼顾local和global信息，即文中说的combining what and where，取得了不错的效果提升。FCN-32s为59.4，FCN-16s提升到了62.4，FCN-8s提升到62.7。可以看出效果还是很明显的。</p>
  </li>
  <li>
    <p>3.训练细节</p>

    <ol>
      <li>用AlexNet，VGG16或者GoogleNet训练好的模型做初始化，在这个基础上做fine-tuning，全部都fine-tuning。</li>
      <li>采用whole image做训练，不进行patch wise sampling。实验证明直接用全图已经很effective and efficient。</li>
      <li>对class score的卷积层做全零初始化。随机初始化在性能和收敛上没有优势。</li>
    </ol>
  </li>
</ul>

<center><img src="/images/pdMask/fcn3.jpg" /></center>

<h3 id="section-3">优缺点</h3>

<p>优点：</p>

<ol>
  <li>训练一个end-to-end的FCN模型，利用卷积神经网络的很强的学习能力，得到较准确的结果，以前的基于CNN的方法都是要对输入或者输出做一些处理，才能得到最终结果。</li>
  <li>直接使用现有的CNN网络，如AlexNet, VGG16, GoogLeNet，只需在末尾加上upsampling，参数的学习还是利用CNN本身的反向传播原理，”whole image training is effective and efficient.”</li>
  <li>不限制输入图片的尺寸，不要求图片集中所有图片都是同样尺寸，只需在最后upsampling时按原图被subsampling的比例缩放回来，最后都会输出一张与原图大小一致的dense prediction map。</li>
</ol>

<p>缺点：</p>

<ul>
  <li>容易丢失小目标。</li>
</ul>


  </div>
</div>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<!--<a href="https://twitter.com/share" class="twitter-share-button " data-size="small" data-count="none">Tweet</a>
<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
-->
<ul class="pager">

  <li class="previous"><a href="/fdetect/2016/03/02/OverFeat.html">&larr; Older</a></li>


  <li class="next"><a href="/falign/2016/06/13/OpenFace.html">Newer &rarr;</a></li>

</ul>

<!--<div id="disqus_thread">

  <script type="text/javascript">
  /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
      var disqus_shortname = 'diqus-username'; // required: replace example with your forum shortname

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
  </script>
  <noscript>
    Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>

  <a href="http://disqus.com" class="dsq-brlink">
    comments powered by <span class="logo-disqus">Disqus</span>
  </a>

</div>
-->

      </div>
      <div class="container">
  <footer>
    <p class="text-muted credit">
      <center>
      &copy; 2017 cjmcv &middot;
      <a href="https://github.com/cjmcv"> Github Site </a>
      2017-11-13 22:38:20 +0800
      </center>
    </p>
  </footer>
</div>

    </div>
    <!-- Bootstrap core JavaScript-->
<script src="/js/jquery-1.10.2.min.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/hc.js"></script>

  </body>
</html>
